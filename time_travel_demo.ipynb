{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "191df5e9-5a1d-418f-b4ff-004398ef4b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a808494-d33c-4088-bf63-3b9dac242494",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Set AWS region environment variable\n",
    "os.environ['AWS_REGION'] = 'us-west-2'  # Replace with your AWS region\n",
    "os.environ['AWS_ACCESS_KEY_ID'] = 'ADD_AWS_ACCESS_KEY'\n",
    "os.environ['AWS_SECRET_ACCESS_KEY'] = 'ADD_AWS_SECRET_KEY'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "356d775d-c0f4-4207-89cf-00ece44570e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark session with Glue catalog configuration\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"IcebergAssetMaintenance\") \\\n",
    "    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n",
    "    .config(\"spark.jars.packages\", \n",
    "            \"org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.5.2,\" +\n",
    "            \"org.apache.iceberg:iceberg-aws:1.5.2,\" + \n",
    "            \"software.amazon.awssdk:bundle:2.17.89,\" + \n",
    "            \"software.amazon.awssdk:url-connection-client:2.17.89\") \\\n",
    "    .config(\"spark.sql.defaultCatalog\", \"glue_catalog\") \\\n",
    "    .config(\"spark.sql.catalog.glue_catalog\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
    "    .config(\"spark.sql.catalog.glue_catalog.warehouse\", \"s3://dealership-demo/blame_demo\") \\\n",
    "    .config(\"spark.sql.catalog.glue_catalog.catalog-impl\", \"org.apache.iceberg.aws.glue.GlueCatalog\") \\\n",
    "    .config(\"spark.sql.catalog.glue_catalog.io-impl\", \"org.apache.iceberg.aws.s3.S3FileIO\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", 'AWS_ACCESS_KEY') \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", 'AWS_SECRET_KEY') \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"s3.amazonaws.com\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "448c82e1-e469-4b4c-8eac-cb4cfc20f4b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "org.apache.iceberg.spark.SparkCatalog\n",
      "s3://dealership-demo/blame_demo\n",
      "org.apache.iceberg.aws.glue.GlueCatalog\n",
      "org.apache.iceberg.aws.s3.S3FileIO\n",
      "<pyspark.sql.session.SparkSession object at 0xffff899c4490>\n"
     ]
    }
   ],
   "source": [
    "# Print the Spark session details to verify catalog configuration\n",
    "print(spark.conf.get(\"spark.sql.catalog.glue_catalog\"))\n",
    "#print(spark.conf.get(\"spark.sql.catalog.glue_catalog.type\"))\n",
    "print(spark.conf.get(\"spark.sql.catalog.glue_catalog.warehouse\"))\n",
    "print(spark.conf.get(\"spark.sql.catalog.glue_catalog.catalog-impl\"))\n",
    "print(spark.conf.get(\"spark.sql.catalog.glue_catalog.io-impl\"))\n",
    "print(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b46fc2b-03e5-4c10-b9b0-bfba0caa6ac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iceberg Package: org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.5.2,org.apache.iceberg:iceberg-aws:1.5.2,software.amazon.awssdk:bundle:2.17.89,software.amazon.awssdk:url-connection-client:2.17.89\n"
     ]
    }
   ],
   "source": [
    "# Print the configuration to check the Iceberg version\n",
    "print(\"Iceberg Package:\", spark.conf.get(\"spark.jars.packages\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81fe7115-b58c-4618-be5e-5c2d7011a36c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|      catalog|\n",
      "+-------------+\n",
      "| glue_catalog|\n",
      "|spark_catalog|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# List databases in the Glue catalog\n",
    "try:\n",
    "    databases = spark.sql(\"SHOW CATALOGS\")\n",
    "    databases.show()\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e7355cf-db67-4917-ba0e-9cf1dc11fcd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "| namespace|\n",
      "+----------+\n",
      "|blame_demo|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# List databases in the default Glue catalog\n",
    "try:\n",
    "    databases = spark.sql(\"SHOW DATABASES\")\n",
    "    databases.show()\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "27d317f5-0657-47c3-9bf9-a430a6280d3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------+-----------+\n",
      "| namespace|        tableName|isTemporary|\n",
      "+----------+-----------------+-----------+\n",
      "|blame_demo|asset_maintenance|      false|\n",
      "|blame_demo|           assets|      false|\n",
      "|blame_demo| real_estate_data|      false|\n",
      "+----------+-----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# List tables in the glue_catalog.iceberg database\n",
    "try:\n",
    "    tables = spark.sql(\"SHOW TABLES IN glue_catalog.blame_demo\")\n",
    "    tables.show()\n",
    "except Exception as e:\n",
    "    print(f\"Error listing tables: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e749ef0b-326d-4b0f-b6eb-3acdc3a7f882",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run a Query to create a Glue table with the Iceberg table format if it doesn't already exist\n",
    "try:\n",
    "    spark.sql(\"CREATE TABLE IF NOT EXISTS glue_catalog.blame_demo.assets (name string) USING iceberg;\")\n",
    "except Exception as e:\n",
    "    print(f\"Error in creating 'asset' table: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cb04281b-1d51-43fe-a315-c750c4e4fe13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------+-----------+\n",
      "| namespace|        tableName|isTemporary|\n",
      "+----------+-----------------+-----------+\n",
      "|blame_demo|asset_maintenance|      false|\n",
      "|blame_demo|           assets|      false|\n",
      "|blame_demo| real_estate_data|      false|\n",
      "+----------+-----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#check if table creation worked\n",
    "try:\n",
    "    tables = spark.sql(\"SHOW TABLES IN glue_catalog.blame_demo\")\n",
    "    tables.show()\n",
    "except Exception as e:\n",
    "    print(f\"Error checking for table existence: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "177bc8cb-0a03-42aa-9914-d61ef0773858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the schema\n",
    "schema = StructType([\n",
    "    StructField(\"asset_id\", StringType(), True),\n",
    "    StructField(\"asset_type\", StringType(), True),\n",
    "    StructField(\"maintenance_date\", TimestampType(), True),\n",
    "    StructField(\"invoice_date\", TimestampType(), True),\n",
    "    StructField(\"issue\", IntegerType(), True),\n",
    "    StructField(\"cost\", IntegerType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1bea21a-b4ab-4ffe-9b44-c057d5b7b93e",
   "metadata": {},
   "source": [
    "# Class for Asset data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5ecc6295-4e1e-4b10-83f8-0b3dc985c2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Asset:\n",
    "    issues_map = {\"Inspection\": 1, \"Repair\": 2, \"Replacement\": 3}\n",
    "\n",
    "    def __init__(self, asset_type, start_date, zero_cost_percentage):\n",
    "        self.asset_id = f\"{asset_type[:3].upper()}_{random.randint(1000, 9999)}\"\n",
    "        self.asset_type = asset_type\n",
    "        \n",
    "        # Ensure maintenance_date is within the same month as start_date\n",
    "        days_in_month = (start_date.replace(month=start_date.month % 12 + 1, day=1) - timedelta(days=1)).day\n",
    "        self.maintenance_date = start_date + timedelta(days=random.randint(0, days_in_month - start_date.day))\n",
    "\n",
    "        # Invoice date is a random number of days (0 to 21) after maintenance date\n",
    "        self.invoice_date = self.maintenance_date + timedelta(days=random.randint(0, 21))\n",
    "        \n",
    "        self.issue = self.issues_map[random.choice(list(self.issues_map.keys()))]\n",
    "\n",
    "        #Set 10% of costs to zero, the rest are random.\n",
    "        if random.random() < zero_cost_percentage:\n",
    "            self.cost = 0\n",
    "        else:\n",
    "            self.cost = random.randint(100, 1000)\n",
    "\n",
    "    def to_dict(self):\n",
    "        return {\n",
    "            \"asset_id\": self.asset_id,\n",
    "            \"asset_type\": self.asset_type,\n",
    "            \"maintenance_date\": self.maintenance_date,\n",
    "            \"invoice_date\": self.invoice_date,\n",
    "            \"issue\": self.issue,\n",
    "            \"cost\": self.cost\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9943290e-2f27-4de0-a32b-a39ca1896888",
   "metadata": {},
   "source": [
    "# Simulate a file with good data being loaded to Iceberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "86052ffa-e695-402b-ad41-cf55107ae2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sample data\n",
    "asset_types = [\"HVAC\", \"Vehicle\", \"Refrigeration\", \"POS\"]\n",
    "start_date = datetime(2024, 1, 1)\n",
    "zero_cost_percentage = 0.1\n",
    "\n",
    "data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e5b372b9-554f-4382-8a35-a5b3e8f72749",
   "metadata": {},
   "outputs": [],
   "source": [
    "for asset_type in asset_types:\n",
    "    for _ in range(250):\n",
    "        asset = Asset(asset_type, start_date, zero_cost_percentage)\n",
    "        data.append(asset.to_dict())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3ac35ff9-87aa-4d9d-9457-a8d7d93beffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "29ff9cd8-d4ed-460a-8d45-ac2ab4649ca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-------------------+-------------------+-----+----+\n",
      "|asset_id|asset_type|   maintenance_date|       invoice_date|issue|cost|\n",
      "+--------+----------+-------------------+-------------------+-----+----+\n",
      "|HVA_9093|      HVAC|2024-01-03 00:00:00|2024-01-03 00:00:00|    3|   0|\n",
      "|HVA_4159|      HVAC|2024-01-26 00:00:00|2024-01-31 00:00:00|    3| 510|\n",
      "|HVA_3081|      HVAC|2024-01-21 00:00:00|2024-01-29 00:00:00|    2| 687|\n",
      "|HVA_2811|      HVAC|2024-01-09 00:00:00|2024-01-14 00:00:00|    1| 518|\n",
      "|HVA_9942|      HVAC|2024-01-14 00:00:00|2024-02-03 00:00:00|    2| 894|\n",
      "|HVA_7672|      HVAC|2024-01-17 00:00:00|2024-01-20 00:00:00|    2| 739|\n",
      "|HVA_4783|      HVAC|2024-01-23 00:00:00|2024-02-02 00:00:00|    2| 378|\n",
      "|HVA_3768|      HVAC|2024-01-23 00:00:00|2024-02-03 00:00:00|    2|   0|\n",
      "|HVA_5687|      HVAC|2024-01-01 00:00:00|2024-01-20 00:00:00|    3|   0|\n",
      "|HVA_2257|      HVAC|2024-01-13 00:00:00|2024-01-18 00:00:00|    2|   0|\n",
      "+--------+----------+-------------------+-------------------+-----+----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "21b886b8-862e-4a1d-8ba9-d85c2a8ff662",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Create the Iceberg table with partitioning\n",
    "spark.sql(\"\"\"\n",
    "CREATE OR REPLACE TABLE glue_catalog.blame_demo.asset_maintenance (\n",
    "    asset_id STRING,\n",
    "    asset_type STRING,\n",
    "    maintenance_date TIMESTAMP,\n",
    "    invoice_date TIMESTAMP,\n",
    "    issue INT,\n",
    "    cost INT\n",
    ")\n",
    "USING iceberg\n",
    "PARTITIONED BY (asset_type, months(invoice_date))\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "84c7b015-fb11-4208-b594-563bf1819887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write DataFrame to the Iceberg table\n",
    "df.write.format(\"iceberg\").mode(\"overwrite\").save(\"glue_catalog.blame_demo.asset_maintenance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37f6cc6-f302-45b8-9118-5f4cf5bee4d0",
   "metadata": {},
   "source": [
    "# Simulate a file with bad data being loaded  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "46cd5ca9-2f2c-486d-8283-b2101f938162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sample of bad data\n",
    "asset_types = [\"HVAC\", \"Vehicle\", \"Refrigeration\", \"POS\"]\n",
    "start_date = datetime(2024, 2, 1)\n",
    "zero_cost_percentage = 0.5\n",
    "\n",
    "data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ecf02252-0e81-4c5c-99ce-dfb787daa01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for asset_type in asset_types:\n",
    "    for _ in range(250):\n",
    "        asset = Asset(asset_type, start_date, zero_cost_percentage)\n",
    "        data.append(asset.to_dict())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9af450ca-6e07-40b3-b6d6-2dbd1cbfd500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d3b5a167-37b9-4acf-9f16-89bc03130732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append DataFrame to the Iceberg table\n",
    "df.write.format(\"iceberg\").mode(\"append\").save(\"glue_catalog.blame_demo.asset_maintenance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "935d17bb-f16e-4948-89b0-054c38d3fc44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-------------------+-------------------+-----+----+\n",
      "|asset_id|asset_type|   maintenance_date|       invoice_date|issue|cost|\n",
      "+--------+----------+-------------------+-------------------+-----+----+\n",
      "|VEH_4466|   Vehicle|2024-02-14 00:00:00|2024-03-02 00:00:00|    2|   0|\n",
      "|VEH_9868|   Vehicle|2024-02-29 00:00:00|2024-03-16 00:00:00|    2| 148|\n",
      "|VEH_1653|   Vehicle|2024-02-10 00:00:00|2024-03-01 00:00:00|    2|   0|\n",
      "|VEH_1338|   Vehicle|2024-02-18 00:00:00|2024-03-06 00:00:00|    1| 446|\n",
      "|VEH_3325|   Vehicle|2024-02-29 00:00:00|2024-03-15 00:00:00|    3| 868|\n",
      "|VEH_4410|   Vehicle|2024-02-20 00:00:00|2024-03-02 00:00:00|    3|   0|\n",
      "|VEH_3234|   Vehicle|2024-02-21 00:00:00|2024-03-04 00:00:00|    3|   0|\n",
      "|VEH_5695|   Vehicle|2024-02-26 00:00:00|2024-03-10 00:00:00|    1| 533|\n",
      "|VEH_8597|   Vehicle|2024-02-25 00:00:00|2024-03-07 00:00:00|    3| 567|\n",
      "|VEH_4878|   Vehicle|2024-02-28 00:00:00|2024-03-03 00:00:00|    2| 783|\n",
      "+--------+----------+-------------------+-------------------+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Execute query to get a sample of 10 rows\n",
    "try:\n",
    "    sample_spark_df = spark.sql(\"SELECT * FROM glue_catalog.blame_demo.asset_maintenance LIMIT 10\")\n",
    "    sample_spark_df.show()\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4a9c4664-7754-4144-b6b2-a979610ba931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+-------------------+-------------------+-----+----+\n",
      "|asset_id|   asset_type|   maintenance_date|       invoice_date|issue|cost|\n",
      "+--------+-------------+-------------------+-------------------+-----+----+\n",
      "|VEH_4019|      Vehicle|2024-02-29 00:00:00|2024-03-21 00:00:00|    3|   0|\n",
      "|REF_5086|Refrigeration|2024-02-29 00:00:00|2024-03-21 00:00:00|    1| 171|\n",
      "|REF_5089|Refrigeration|2024-02-29 00:00:00|2024-03-21 00:00:00|    3|   0|\n",
      "|POS_6446|          POS|2024-02-29 00:00:00|2024-03-21 00:00:00|    3| 175|\n",
      "+--------+-------------+-------------------+-------------------+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the row with the max timestamp\n",
    "try:\n",
    "    max_timestamp = spark.sql(\"\"\"\n",
    "        SELECT * \n",
    "        FROM glue_catalog.blame_demo.asset_maintenance \n",
    "        WHERE invoice_date = (SELECT MAX(invoice_date) FROM glue_catalog.blame_demo.asset_maintenance);\n",
    "    \"\"\")\n",
    "    max_timestamp.show()\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2051e5-8aaa-45da-b6b7-aa5a89236536",
   "metadata": {},
   "source": [
    "# Generate another good sample file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f8c9922f-5e86-422d-a2c8-29fe8eac5e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sample of good data for March\n",
    "asset_types = [\"HVAC\", \"Vehicle\", \"Refrigeration\", \"POS\"]\n",
    "start_date = datetime(2024, 3, 1)\n",
    "zero_cost_percentage = 0.1\n",
    "\n",
    "data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ec563544-2e44-47f5-9a29-b4ef65facb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "for asset_type in asset_types:\n",
    "    for _ in range(250):\n",
    "        asset = Asset(asset_type, start_date, zero_cost_percentage)\n",
    "        data.append(asset.to_dict())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "77768998-e485-4b45-be90-c90231da2381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "36c272f2-68ab-44e3-9843-3634cc6a13e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append DataFrame to the Iceberg table\n",
    "df.write.format(\"iceberg\").mode(\"append\").save(\"glue_catalog.blame_demo.asset_maintenance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "825906ed-bc94-4da0-82a0-5d3cf78727c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+-------------------+-------------------+---------+-------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|committed_at           |snapshot_id        |parent_id          |operation|manifest_list                                                                                                                  |summary                                                                                                                                                                                                                                                                                                      |\n",
      "+-----------------------+-------------------+-------------------+---------+-------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|2024-07-01 06:02:21.178|1291093834306420945|NULL               |overwrite|s3://dealership-demo/blame_demo/asset_maintenance/metadata/snap-1291093834306420945-1-952f50b2-e59c-484e-94c9-753d924a8552.avro|{spark.app.id -> local-1719749580589, added-data-files -> 52, added-records -> 1000, added-files-size -> 120062, changed-partition-count -> 52, total-records -> 1000, total-files-size -> 120062, total-data-files -> 52, total-delete-files -> 0, total-position-deletes -> 0, total-equality-deletes -> 0}|\n",
      "|2024-07-01 06:40:46.55 |7046654947850004437|NULL               |overwrite|s3://dealership-demo/blame_demo/asset_maintenance/metadata/snap-7046654947850004437-1-6be808d2-1f5f-48bd-b905-7166868c52c4.avro|{spark.app.id -> local-1719749580589, added-data-files -> 52, added-records -> 2000, added-files-size -> 128132, changed-partition-count -> 52, total-records -> 2000, total-files-size -> 128132, total-data-files -> 52, total-delete-files -> 0, total-position-deletes -> 0, total-equality-deletes -> 0}|\n",
      "|2024-07-01 06:57:53.009|2047182808623583161|NULL               |overwrite|s3://dealership-demo/blame_demo/asset_maintenance/metadata/snap-2047182808623583161-1-7bb8b214-6d5d-49ce-9772-85a76dc19a0b.avro|{spark.app.id -> local-1719816991521, added-data-files -> 8, added-records -> 1000, added-files-size -> 24852, changed-partition-count -> 8, total-records -> 1000, total-files-size -> 24852, total-data-files -> 8, total-delete-files -> 0, total-position-deletes -> 0, total-equality-deletes -> 0}     |\n",
      "|2024-07-01 07:07:11.404|6982677708530941242|2047182808623583161|append   |s3://dealership-demo/blame_demo/asset_maintenance/metadata/snap-6982677708530941242-1-94cfcd4c-c903-4092-8f0e-29a8dfc316b7.avro|{spark.app.id -> local-1719816991521, added-data-files -> 8, added-records -> 1000, added-files-size -> 24972, changed-partition-count -> 8, total-records -> 2000, total-files-size -> 49824, total-data-files -> 16, total-delete-files -> 0, total-position-deletes -> 0, total-equality-deletes -> 0}    |\n",
      "|2024-07-01 07:14:45.574|1182465130147489411|6982677708530941242|append   |s3://dealership-demo/blame_demo/asset_maintenance/metadata/snap-1182465130147489411-1-fe47d281-b55f-4a67-b833-a3dfd0d8c69f.avro|{spark.app.id -> local-1719816991521, added-data-files -> 8, added-records -> 1000, added-files-size -> 24836, changed-partition-count -> 8, total-records -> 3000, total-files-size -> 74660, total-data-files -> 24, total-delete-files -> 0, total-position-deletes -> 0, total-equality-deletes -> 0}    |\n",
      "+-----------------------+-------------------+-------------------+---------+-------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# List all snapshots of the Iceberg table\n",
    "snapshots_df = spark.sql(\"SELECT * FROM glue_catalog.blame_demo.asset_maintenance.snapshots\")\n",
    "snapshots_df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9ce355db-a668-4847-9239-bef6f9445e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "snapshot_id = '6982677708530941242'\n",
    "\n",
    "# Run the time travel query using the snapshot ID\n",
    "data_at_bad_snapshot = spark.sql(f\"\"\"\n",
    "    SELECT * FROM glue_catalog.blame_demo.asset_maintenance\n",
    "    FOR SYSTEM_VERSION AS OF {snapshot_id}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "03693205-d533-4ee3-8a33-3a71e669c4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_df = data_at_bad_snapshot.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "05b79c1d-54d1-406d-84bc-2c241d24e254",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asset_id</th>\n",
       "      <th>asset_type</th>\n",
       "      <th>maintenance_date</th>\n",
       "      <th>invoice_date</th>\n",
       "      <th>issue</th>\n",
       "      <th>cost</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>VEH_4466</td>\n",
       "      <td>Vehicle</td>\n",
       "      <td>2024-02-14</td>\n",
       "      <td>2024-03-02</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>VEH_9868</td>\n",
       "      <td>Vehicle</td>\n",
       "      <td>2024-02-29</td>\n",
       "      <td>2024-03-16</td>\n",
       "      <td>2</td>\n",
       "      <td>148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>VEH_1653</td>\n",
       "      <td>Vehicle</td>\n",
       "      <td>2024-02-10</td>\n",
       "      <td>2024-03-01</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>VEH_1338</td>\n",
       "      <td>Vehicle</td>\n",
       "      <td>2024-02-18</td>\n",
       "      <td>2024-03-06</td>\n",
       "      <td>1</td>\n",
       "      <td>446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>VEH_3325</td>\n",
       "      <td>Vehicle</td>\n",
       "      <td>2024-02-29</td>\n",
       "      <td>2024-03-15</td>\n",
       "      <td>3</td>\n",
       "      <td>868</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   asset_id asset_type maintenance_date invoice_date  issue  cost\n",
       "0  VEH_4466    Vehicle       2024-02-14   2024-03-02      2     0\n",
       "1  VEH_9868    Vehicle       2024-02-29   2024-03-16      2   148\n",
       "2  VEH_1653    Vehicle       2024-02-10   2024-03-01      2     0\n",
       "3  VEH_1338    Vehicle       2024-02-18   2024-03-06      1   446\n",
       "4  VEH_3325    Vehicle       2024-02-29   2024-03-15      3   868"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b6041de8-2c04-417f-936f-7286c96b1a3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.306\n"
     ]
    }
   ],
   "source": [
    "print((p_df['cost'] == 0).sum()/p_df['cost'].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2b3f5e-46fa-482a-b8bc-144fa2b7dc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rollback to previous snapshot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "befb6c53-99f6-4884-8309-f355252abc04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[previous_snapshot_id: bigint, current_snapshot_id: bigint]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Roll back the table to the identified snapshot\n",
    "spark.sql(f\"\"\"\n",
    "    CALL system.rollback_to_snapshot(\n",
    "        'glue_catalog.blame_demo.asset_maintenance',\n",
    "        {snapshot_id}\n",
    "    )\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "910ba9e1-6d64-4410-b494-e4c79d741983",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+-------------------+-------------------+-------------------+\n",
      "|made_current_at        |snapshot_id        |parent_id          |is_current_ancestor|\n",
      "+-----------------------+-------------------+-------------------+-------------------+\n",
      "|2024-07-01 06:02:21.178|1291093834306420945|NULL               |false              |\n",
      "|2024-07-01 06:40:46.55 |7046654947850004437|NULL               |false              |\n",
      "|2024-07-01 06:57:53.009|2047182808623583161|NULL               |true               |\n",
      "|2024-07-01 07:07:11.404|6982677708530941242|2047182808623583161|true               |\n",
      "|2024-07-01 07:14:45.574|1182465130147489411|6982677708530941242|false              |\n",
      "|2024-07-01 11:13:58.457|6982677708530941242|2047182808623583161|true               |\n",
      "+-----------------------+-------------------+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Validate the roll back by checking the table history\n",
    "current_snapshot = spark.sql(\"SELECT * FROM glue_catalog.blame_demo.asset_maintenance.history\")\n",
    "current_snapshot.show(truncate=False)\n",
    "\n",
    "#in the output we can see that '6982677708530941242' was generated, then it became the parent of the next snapshot, then it was recorded again, signally a roll back.."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
